{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcB+XiArNj6LWjLcTaa7ng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hailemicael/NLP_SECOND_ASSIGNEMNT/blob/master/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Importing Libraries and Reading Input\n"
      ],
      "metadata": {
        "id": "QdKWAIFpe9Je"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M3tv4QoeCbE",
        "outputId": "21967e20-61a9-4e2e-8356-bcf7bc77e6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read input from file\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    input_text = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Preprocessing Function"
      ],
      "metadata": {
        "id": "iO3MghxJfBQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize words and handle punctuation and numbers\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum()]\n",
        "\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(clean_text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return \" \".join(words)\n",
        "# Tokenize and preprocess\n",
        "processed_input = preprocess_text(input_text)"
      ],
      "metadata": {
        "id": "XIgphUa5ebfk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3:  Context Window Slicing Algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "jt708P6AfJ4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Context Window Slicing Algorithm\n",
        "def generate_slices(input_text, context_window_size=128):\n",
        "    # Convert context_window_size to bytes\n",
        "    context_window_bytes = context_window_size * 1024 * 1024\n",
        "\n",
        "\n",
        "    # No Slice is needed in this case\n",
        "    if len(input_text.encode('utf-8')) <= context_window_bytes:\n",
        "        return [input_text]\n",
        "\n",
        "\n",
        "\n",
        "    # Split into slices ensuring complete words\n",
        "    slice_size = context_window_bytes\n",
        "    words = processed_input.split()\n",
        "    slices = []\n",
        "    current_slice = \"\"\n",
        "\n",
        "    for word in words:\n",
        "        if len(current_slice.encode('utf-8')) + len(word.encode('utf-8')) <= slice_size:\n",
        "            current_slice += \" \" + word\n",
        "        else:\n",
        "            slices.append(current_slice.strip())\n",
        "            current_slice = word\n",
        "\n",
        "    if current_slice:\n",
        "        slices.append(current_slice.strip())\n",
        "\n",
        "    # Ensure slices meet criteria\n",
        "    final_slices = [slices[0]]\n",
        "    for i in range(1, len(slices)):\n",
        "        # Compare adjacent slices using cosine similarity\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform([final_slices[-1], slices[i]])\n",
        "        cosine_distance = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "        # Print debugging information\n",
        "        print(f\"Slice {i + 1}: Cosine Similarity = {cosine_distance}\")\n",
        "\n",
        "\n",
        "        # Adjust this threshold as needed (20%)\n",
        "        if cosine_distance > 0.2:\n",
        "            final_slices.append(slices[i])\n",
        "\n",
        "    return final_slices\n"
      ],
      "metadata": {
        "id": "Qpun_qGZexHv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4: Generate Slices and save to new text file\n"
      ],
      "metadata": {
        "id": "7TW4fDtrjuT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate slices\n",
        "slices = generate_slices(input_text)\n",
        "\n",
        "# Save slices to a file\n",
        "with open('slices_output.txt', 'w', encoding='utf-8') as output_file:\n",
        "    for i, slice_text in enumerate(slices):\n",
        "        output_file.write(f\"Slice {i + 1}: {slice_text}\\n\")\n"
      ],
      "metadata": {
        "id": "phs5VqZIjexQ"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}