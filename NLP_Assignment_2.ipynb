{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEAf2Tc4cB4lrNFBBm8YXO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hailemicael/NLP_SECOND_ASSIGNEMNT/blob/master/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Importing Libraries and Reading Input**\n"
      ],
      "metadata": {
        "id": "QdKWAIFpe9Je"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M3tv4QoeCbE",
        "outputId": "0fb4e892-4e50-4ebd-cd43-450e2cf42f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read input from file\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    input_text = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Preprocessing Function"
      ],
      "metadata": {
        "id": "iO3MghxJfBQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize words and handle punctuation and numbers\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum()]\n",
        "\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(clean_text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return \" \".join(words)\n",
        "# Tokenize and preprocess\n",
        "processed_input = preprocess_text(input_text)"
      ],
      "metadata": {
        "id": "XIgphUa5ebfk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3:  Context Window Slicing Algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "jt708P6AfJ4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Context Window Slicing Algorithm\n",
        "def generate_slices(input_text, context_window_size=128):\n",
        "    # Convert context_window_size to bytes\n",
        "    context_window_bytes = context_window_size * 1024 * 1024\n",
        "\n",
        "\n",
        "    # No Slice is needed in this case\n",
        "    if len(input_text.encode('utf-8')) <= context_window_bytes:\n",
        "        return [input_text]\n",
        "\n",
        "\n",
        "\n",
        "    # Split into slices ensuring complete words\n",
        "    slice_size = context_window_bytes\n",
        "    words = processed_input.split()\n",
        "    slices = []\n",
        "    current_slice = \"\"\n",
        "\n",
        "    for word in words:\n",
        "        if len(current_slice.encode('utf-8')) + len(word.encode('utf-8')) <= slice_size:\n",
        "            current_slice += \" \" + word\n",
        "        else:\n",
        "            slices.append(current_slice.strip())\n",
        "            current_slice = word\n",
        "\n",
        "    if current_slice:\n",
        "        slices.append(current_slice.strip())\n",
        "\n",
        "    # Ensure slices meet criteria\n",
        "    final_slices = [slices[0]]\n",
        "    for i in range(1, len(slices)):\n",
        "        # Compare adjacent slices using cosine similarity\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform([final_slices[-1], slices[i]])\n",
        "        cosine_distance = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "        # Print debugging information\n",
        "        print(f\"Slice {i + 1}: Cosine Similarity = {cosine_distance}\")\n",
        "\n",
        "\n",
        "        # Adjust the threshold\n",
        "        if cosine_distance > 0.2:\n",
        "            final_slices.append(slices[i])\n",
        "\n",
        "    return final_slices\n"
      ],
      "metadata": {
        "id": "Qpun_qGZexHv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4: Generate Slices and save to new text file\n"
      ],
      "metadata": {
        "id": "7TW4fDtrjuT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate slices\n",
        "slices = generate_slices(input_text)\n",
        "\n",
        "# Save slices to a file\n",
        "with open('slices_output.txt', 'w', encoding='utf-8') as output_file:\n",
        "    for i, slice_text in enumerate(slices):\n",
        "        output_file.write(f\"Slice {i + 1}: {slice_text}\\n\")\n"
      ],
      "metadata": {
        "id": "phs5VqZIjexQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Feeding the slice text to the Model**"
      ],
      "metadata": {
        "id": "bz2smcjsP8Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install the replicate library\n",
        "!pip install replicate"
      ],
      "metadata": {
        "id": "qRNdI2RkQXTe",
        "outputId": "d7c75893-e8b8-4cae-8f03-a51cf34f6034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: replicate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (0.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (23.2)\n",
            "Requirement already satisfied: pydantic>1 in /usr/local/lib/python3.10/dist-packages (from replicate) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-1 Import necessary libraries, Authenticate with Replicate API, define the model and read slice text.\n"
      ],
      "metadata": {
        "id": "KSYbe1rVRu36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate\n",
        "REPLICATE_API_TOKEN = \"r8_e9k5S3RuqDisxjw5p2S8PnSLoS8Nd0t10j8og\"\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
        "\n",
        "model_name = \"meta/llama-2-70b-chat\"\n",
        "\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:\n",
        "    slice_text = input_file.read()"
      ],
      "metadata": {
        "id": "HWEcDGepROQ1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-2: Providing the input sliced text, asking question based on the stored input and run the model.\n"
      ],
      "metadata": {
        "id": "oDjkShPnSkXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide initial input to the model\n",
        "for event in client.stream(\n",
        "    model_name,\n",
        "    input={\n",
        "        \"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input:\"\n",
        "    },\n",
        "):\n",
        "    user_input = str(event)\n",
        "\n",
        "#  Ask questions based on the stored input\n",
        "user_question = input(\"You: \")\n",
        "\n",
        "# Run the model with the stored input and the user's question\n",
        "for event in client.stream(\n",
        "    model_name,\n",
        "    input={\n",
        "        \"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"\n",
        "    },\n",
        "):\n",
        "    print(str(event), end=\"\")\n"
      ],
      "metadata": {
        "id": "yfSMlMfnSD-y",
        "outputId": "82dbf1e5-9722-42b0-b953-261210ceff79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: what is philosophy \n",
            "\n",
            "Philosophy is a field of study that involves critical thinking, analysis, and systematic inquiry into fundamental questions about existence, reality, knowledge, values, reason, and ethics. It is a discipline that seeks to understand and clarify the nature of things, the meaning of life, and the principles that govern human conduct. Philosophers often engage in debates, discussions, and arguments about various topics, and they use logical reasoning, empirical evidence, and historical context to support their views.\n",
            "\n",
            "Philosophy encompasses many subfields, including metaphysics, epistemology,"
          ]
        }
      ]
    }
  ]
}